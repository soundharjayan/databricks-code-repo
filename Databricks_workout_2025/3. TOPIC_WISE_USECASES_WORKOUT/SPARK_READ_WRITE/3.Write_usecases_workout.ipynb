{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e1d6d88-7bcc-4548-a0d1-15d37f6fc0be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Spark Write Operations using \n",
    "- csv, json, orc, parquet, delta, saveAsTable, insertInto, xml with different write mode, header and sep options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e34c3bc-962d-438d-a1b6-ac27d2da6608",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1. Write Operations (Data Conversion/Schema migration) – CSV Format Usecases\n",
    "1. Write customer data into CSV format using overwrite mode\n",
    "2. Write usage data into CSV format using append mode\n",
    "3. Write tower data into CSV format with header enabled and custom separator (|)\n",
    "4. Read the tower data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24e68923-331e-4d6d-95d7-6401c63e0ae5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading the CSV file and storing it in a datframe \n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "custom_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"customer_age\", IntegerType(), True),\n",
    "    StructField(\"customer_city\", StringType(), True),\n",
    "    StructField(\"customer_plan_type\", StringType(), True)])\n",
    "\n",
    "read_customer_df = spark.read.schema(custom_schema).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "\n",
    "# 1.Write customer data into CSV format using overwrite mode\n",
    "write_customer_csv_df = read_customer_df.write.options(header='true').mode(\"overwrite\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/csvout/\")\n",
    "display(write_customer_csv_df)\n",
    "\n",
    "# 2.Write usage data into CSV format using append mode\n",
    "write_customer_csv_df = read_customer_df.write.options(header='true').mode(\"append\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/csvout/\")\n",
    "display(write_customer_csv_df)\n",
    "\n",
    "# 3.Write tower data into CSV format with header enabled and custom separator (|)\n",
    "read_tower_df = spark.read.options(header='true',sep='|',inferSchema='true',pathGlobeFilter='.csv',recursiveFileLookup='true').format('csv').load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region*\")\n",
    "display(read_tower_df)\n",
    "\n",
    "write_tower_csv_df = read_tower_df.write.options(header='true',sep='|').mode(\"overwrite\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/csvout/\")\n",
    "display(write_tower_csv_df)\n",
    "\n",
    "# 4.Read the tower data in a dataframe and show only 5 rows.\n",
    "display(read_tower_df.limit(5))\n",
    "\n",
    "# 5.Download the file into local from the catalog volume location and see the data of any of the above files opening in a notepad++.\n",
    "'''Yes, I could download the file into local from the catalog volume location and see the data of above files opening in a notepad++.'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34158cf6-dd7f-40d6-9969-ed76710540a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Write Operations (Data Conversion/Schema migration)– JSON Format Usecases\n",
    "1. Write customer data into JSON format using overwrite mode\n",
    "2. Write usage data into JSON format using append mode and snappy compression format\n",
    "3. Write tower data into JSON format using ignore mode and observe the behavior of this mode\n",
    "4. Read the tower data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "654d505d-072c-402f-9f84-f98cb93f8164",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1.Write customer data into JSON format using overwrite mode\n",
    "write_customer_json_df = read_customer_df.write.mode(\"overwrite\").json(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/jsonout/\")\n",
    "\n",
    "#2.Write usage data into JSON format using append mode and snappy compression format\n",
    "read_usage_csv_df = spark.read.options(header= 'true',inferSchema=\"True\",sep ='\\t').csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\")\n",
    "display(read_usage_csv_df)\n",
    "\n",
    "write_usage_json_df = read_usage_csv_df.write.mode(\"append\").option(\"compression\",\"snappy\").json(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/jsonout/\")\n",
    "\n",
    "#3.Write tower data into JSON format using ignore mode and observe the behavior of this mode\n",
    "write_tower_json_df = read_tower_df.write.mode(\"ignore\").json(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/jsonout/\")\n",
    "\n",
    "#4.Read the tower data in a dataframe and show only 5 rows\n",
    "display(read_tower_df.limit(5))\n",
    "\n",
    "#5.Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++.\n",
    "'''Yes, I was able to download the files locally from the catalog volume location and view the data of all three files using Notepad++. \n",
    "But, out of 3 files, only two were in readable JSON format. The 'usage' file was compressed, so I was unable to view the data in a clear format.'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26f2ba69-3cde-4ec6-8945-e4ef9f7bb109",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Write Operations (Data Conversion/Schema migration) – Parquet Format Usecases\n",
    "1. Write customer data into Parquet format using overwrite mode and in a gzip format\n",
    "2. Write usage data into Parquet format using error mode\n",
    "3. Write tower data into Parquet format with gzip compression option\n",
    "4. Read the usage data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6c99b65-25b4-4f9e-a8eb-3ef7be2b0619",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1.Write customer data into Parquet format using overwrite mode and in a gzip format\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "custom_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"customer_age\", IntegerType(), True),\n",
    "    StructField(\"customer_city\", StringType(), True),\n",
    "    StructField(\"customer_plan_type\", StringType(), True)])\n",
    "\n",
    "read_customer_df = spark.read.schema(custom_schema).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "display(read_customer_df)\n",
    "\n",
    "write_customer_parquet_df = read_customer_df.write.mode(\"overwrite\").option(\"compression\",\"gzip\").parquet(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/parquetout/\")\n",
    "\n",
    "#2.Write usage data into Parquet format using error mode\n",
    "read_usage_csv_df = spark.read.options(header= 'true',inferSchema=\"True\",sep ='\\t').csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\")\n",
    "display(read_usage_csv_df)\n",
    "\n",
    "write_usage_parquet_df = read_usage_csv_df.write.mode(\"error\").parquet(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/parquetout/\")\n",
    "\n",
    "#3.Write tower data into Parquet format with gzip compression option\n",
    "read_tower_df = spark.read.options(header='true',sep='|',inferSchema='true',pathGlobeFilter='.csv',recursiveFileLookup='true').format('csv').load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region*\")\n",
    "display(read_tower_df)\n",
    "\n",
    "write_tower_parquet_df = read_tower_df.write.mode(\"overwrite\").option(\"compression\",\"gzip\").parquet(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/parquetout/\")\n",
    "\n",
    "#4.Read the usage data in a dataframe and show only 5 rows.\n",
    "\n",
    "display(read_usage_csv_df.limit(5))\n",
    "\n",
    "#5.Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++.\n",
    "'''Yes, I was able to download the file to my local machine from the catalog volume location, but I couldn’t view the data because it is compressed and stored in parquet format.'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b41c794f-5cfc-4aeb-a599-e6d4a47a0f3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##4. Write Operations (Data Conversion/Schema migration) – Orc Format Usecases\n",
    "1. Write customer data into ORC format using overwrite mode\n",
    "2. Write usage data into ORC format using append mode\n",
    "3. Write tower data into ORC format and see the output file structure\n",
    "4. Read the usage data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7129acd-c2f1-417c-9bcd-fb015d3ca16e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1.Write customer data into ORC format using overwrite mode\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "custom_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"customer_age\", IntegerType(), True),\n",
    "    StructField(\"customer_city\", StringType(), True),\n",
    "    StructField(\"customer_plan_type\", StringType(), True)])\n",
    "\n",
    "read_customer_df = spark.read.schema(custom_schema).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "display(read_customer_df)\n",
    "\n",
    "write_customer_orc_df = read_customer_df.write.mode(\"overwrite\").format('orc').save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/orcout/\")\n",
    "\n",
    "#2.Write usage data into ORC format using append mode\n",
    "read_usage_csv_df = spark.read.options(header= 'true',inferSchema=\"True\",sep ='\\t').csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\")\n",
    "display(read_usage_csv_df)\n",
    "\n",
    "write_usage_orc_df = read_usage_csv_df.write.mode(\"append\").format('orc').save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/orcout/\")\n",
    "\n",
    "#3.Write tower data into ORC format and see the output file structure\n",
    "read_tower_csv_df = spark.read.options(header='true',sep='|',inferSchema='true',pathGlobeFilter='.csv',recursiveFileLookup='true').format('csv').load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region*\")\n",
    "display(read_tower_df)\n",
    "\n",
    "write_tower_orc_df = read_tower_csv_df.write.mode(\"overwrite\").orc(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/orcout/\")\n",
    "\n",
    "#4.Read the usage data in a dataframe and show only 5 row\n",
    "display(read_usage_csv_df.limit(5))\n",
    "\n",
    "#5.Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++\n",
    "'''Yes, I was able to download the file to my local machine from the catalog volume location, but I couldn’t view the data because it is compressed and stored in ORC format.'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35761315-0b0f-46ff-9c3d-c0405bce7b62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##5. Write Operations (Data Conversion/Schema migration) – Delta Format Usecases\n",
    "1. Write customer data into Delta format using overwrite mode\n",
    "2. Write usage data into Delta format using append mode\n",
    "3. Write tower data into Delta format and see the output file structure\n",
    "4. Read the usage data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++.\n",
    "6. Compare the parquet location and delta location and try to understand what is the differentiating factor, as both are parquet files only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4921e5e0-4e2b-492d-b11c-7de143c197a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1.Write customer data into Delta format using overwrite mode\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "custom_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"customer_age\", IntegerType(), True),\n",
    "    StructField(\"customer_city\", StringType(), True),\n",
    "    StructField(\"customer_plan_type\", StringType(), True)])\n",
    "\n",
    "read_customer_df = spark.read.schema(custom_schema).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "display(read_customer_df)\n",
    "\n",
    "write_customer_delta_df = read_customer_df.write.mode(\"overwrite\").format('delta').save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/deltaout/\")\n",
    "\n",
    "\n",
    "#2.Write usage data into Delta format using append mode\n",
    "read_usage_csv_df = spark.read.options(header= 'true',inferSchema=\"True\",sep ='\\t').csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\")\n",
    "display(read_usage_csv_df)\n",
    "\n",
    "write_usage_delta_df = read_usage_csv_df.write.mode(\"append\").format('delta').save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/deltaout/\")\n",
    "\n",
    "#3.Write tower data into Delta format and see the output file structure\n",
    "read_tower_csv_df = spark.read.options(header='true',sep='|',inferSchema='true',pathGlobeFilter='.csv',recursiveFileLookup='true').format('csv').load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region*\")\n",
    "display(read_tower_df)\n",
    "\n",
    "write_tower_delta_df = read_tower_csv_df.write.mode(\"overwrite\").option(\"compression\",\"gzip\").format('delta').save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/deltaout/\")\n",
    "\n",
    "#4.Read the usage data in a dataframe and show only 5 rows.\n",
    "display(read_usage_csv_df.limit(5))\n",
    "\n",
    "#5.Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++.\n",
    "'''I downloaded all the files into local machine from the catolog volume location and I was unable to read the data because it is compressed and stored in delta format (Internally as Parquet format).\n",
    "But I could see the transaction logs in the delta, which are not available in ORC and parquet formats.'''\n",
    "\n",
    "#6.Compare the parquet location and delta location and try to understand what is the differentiating factor, as both are parquet files only.\n",
    "'''The main difference is that the transcation logs are stored in delta format and not in parquet format.\n",
    "The delta format is stored as a parquet format behind the scenes.\n",
    "The delta format is a file format that is optimized for data lakes and is designed to provide efficient\n",
    "We can do the ACID (DML) and Write-many-read-many WMRM activities in delta formats\n",
    "we can't do the above activites in ORC or parquet file formats.We can perform only write-once-read-many WORM activities in ORC'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6dd0890-02bd-4acd-b837-daceb256c706",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##6. Write Operations (Lakehouse Usecases) – Delta table Usecases\n",
    "1. Write customer data using saveAsTable() as a managed table\n",
    "2. Write usage data using saveAsTable() with overwrite mode\n",
    "3. Drop the managed table and verify data removal\n",
    "4. Go and check the table overview and realize it is in delta format in the Catalog.\n",
    "5. Use spark.read.sql to write some simple queries on the above tables created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3967332b-1776-48f3-b1d3-8bc8c3fe21ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1.Write customer data using saveAsTable() as a managed table\n",
    "\n",
    "read_customer_df= spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\").toDF(\"customer_id\",\"customer_name\",\"customer_age\",\"customer_city\",\"customer_plan_type\")\n",
    "\n",
    "write_customer_df = read_customer_df.write.mode('overwrite').format('delta').saveAsTable(\"telecom_catalog_assign.landing_zone.customer_table\")\n",
    "\n",
    "# 2.Write usage data using saveAsTable()\n",
    "\n",
    "read_usage_df = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\",header=True,inferSchema=True,sep='\\t')\n",
    "\n",
    "write_usage_df = read_usage_df.write.format('delta').mode('overwrite').saveAsTable(\"telecom_catalog_assign.landing_zone.usage_table\")\n",
    "\n",
    "# 3.Drop the managed table and verify data removal\n",
    "# %sql\n",
    "# DROP TABLE telecom_catalog_assign.landing_zone.customer_table;\n",
    "\n",
    "# 4.Go and check the table overview and realize it is in delta format in the Catalog.\n",
    "\"1. I've verified, the table has stored as a delta format by default.\"\n",
    "\n",
    "# 5.Use spark.sql to write some simple queries on the above tables created.\n",
    "write_sql= spark.sql(\"select * from telecom_catalog_assign.landing_zone.usage_table\")\n",
    "display(write_sql)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1aac447b-690b-4562-99dd-0ce096e9ad55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##7. Write Operations (Lakehouse Usecases) – Delta table Usecases\n",
    "1. Write customer data using insertInto() in a new table and find the behavior\n",
    "2. Write usage data using insertTable() with append mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5695310a-4626-4817-8904-a333397a7481",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Write customer data using insertInto() in a new table and find the behavior\n",
    "\n",
    "# Read the customer data\n",
    "read_customer_df = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\").toDF(\"customer_id\",\"customer_name\",\"customer_age\",\"customer_city\",\"customer_plan_type\")\n",
    "\n",
    "# Insert into existing table ( it won't write the data into a new target table, because it is doesn't exist in the catalog/schema.It will load the data only if the table is exist)\n",
    "# write_customer_df = read_customer_df.write.insertInto(\"telecom_catalog_assign.landing_zone.cust_table\",overwrite= True)  \n",
    "write_customer_df = read_customer_df.write.insertInto(\"telecom_catalog_assign.landing_zone.customer_table\",overwrite= False)  \n",
    "\n",
    "write_sql= spark.sql(\"select * from telecom_catalog_assign.landing_zone.customer_table\")\n",
    "display(write_sql)\n",
    "\n",
    "# 2. Write usage data using insertInto() with append mode\n",
    " \n",
    "# Read the usage data\n",
    "read_usage_data = spark.read.csv(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    sep='\\t')\n",
    "\n",
    "# Insert into existing table\n",
    "write_usage_data = read_usage_data.write.mode('append').insertInto(\"telecom_catalog_assign.landing_zone.usage_table\")\n",
    "\n",
    "display(spark.sql(\"select * from telecom_catalog_assign.landing_zone.usage_table\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3c4bce3-4bd3-4db6-a074-02bb24c5f91a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##8. Write Operations (Lakehouse Usecases) – Delta table Usecases\n",
    "1. Write customer data into XML format using rowTag as cust\n",
    "2. Write usage data into XML format using overwrite mode with the rowTag as usage\n",
    "3. Download the xml data and open the file in notepad++ and see how the xml file looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5e9abea-f75f-4f43-92a3-3a1275428975",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Write customer data into XML format using rowTag as cust\n",
    "\n",
    "# Read the customer data\n",
    "read_customer_df = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\").toDF(\"customer_id\",\"customer_name\",\"customer_age\",\"customer_city\",\"customer_plan_type\")\n",
    "\n",
    "# Write the data into xml format\n",
    "\n",
    "write_xml_format = read_customer_df.write.format(\"xml\").mode(\"overwrite\").options(rowTag = 'cust').save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/xmlout/\")\n",
    "\n",
    "# 2.Write usage data into XML format using overwrite mode with the rowTag as usage\n",
    "\n",
    "# Read the usage data\n",
    "read_usage_data = spark.read.csv(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    sep='\\t')\n",
    "\n",
    "# Write the data into xml format\n",
    "write_usage_data = read_usage_data.write.mode('overwrite').options(rowTag = 'usage').format('xml').save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/xmlout/\")\n",
    "\n",
    "# 3. Download the xml data and open the file in notepad++ and see how the xml file looks like.\n",
    "\n",
    "\"\"\"I have downloaded the both files and verified in notepad++, it has stored as a xml format with rowtag\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83e2fe69-9352-4ec9-bf70-15d760c89aa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##9. Compare all the downloaded files (csv, json, orc, parquet, delta and xml) \n",
    "1. Capture the size occupied between all of these file formats and list the formats below based on the order of size from small to big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47e9b535-27d7-40f3-af9c-a9eb0c5bf1fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# I've just downloaded a 16mb of csv file and write it as all formats to compare the file sizes. Because we cannot compare and understand the file size difference for small data.\n",
    "\n",
    "read_df = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customers-100000.csv\",header=True,inferSchema=True)\n",
    "\n",
    "write_df = read_df.write.mode('overwrite').format('csv').save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/cout\")\n",
    "write_df1 = read_df.write.mode('overwrite').format('orc').save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/oout\")\n",
    "write_df2 = read_df.write.mode('overwrite').format('parquet').save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/paout\")\n",
    "write_df3 = read_df.write.mode('overwrite').format('json').save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/jout\")\n",
    "\n",
    "read_df2 = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customers-100000.csv\").toDF(\"Index\",\"Customer_Id\", \"First_Name\",\"Company\",\"City\",\"Country\",\n",
    " \"Last_Name\", \"Phone_1\", \"Phone_2\",\"Email\",\"Subscription_Date\",\"Website\")\n",
    "write_df5 = read_df2.write.mode('overwrite').options(rowTag='cust').format('xml').save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/xout\")\n",
    "\n",
    "read_df3 = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customers-100000.csv\").toDF(\"Index\",\"Customer_Id\", \"First_Name\",\"Company\",\"City\",\"Country\",\n",
    " \"Last_Name\", \"Phone_1\", \"Phone_2\",\"Email\",\"Subscription_Date\",\"Website\")\n",
    "write_df4 = read_df3.write.mode('overwrite').format('delta').save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/dout\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccf97269-d4d4-4c8a-bd9a-fa1d9a4a3db3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**1. Capture the size occupied between all of these file formats and list the formats below based on the order of size from small to big.**\n",
    "\n",
    "**_Let me compare customers-100000 data file_**\n",
    "\n",
    "Raw file    => csv     => size - 16.5 MB   <br>\n",
    "format file => csv     => size - 16.44 MB <br>\n",
    "format file => orc     => size - 8.01 MB <br>\n",
    "format file => delta   => size - 9.62 MB <br>\n",
    "format file => parquet => size - 10.13 MB <br>\n",
    "format file => json    => size - 27.09 MB <br>\n",
    "format file => xml     => size - 51.75 MB "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d6e39ec-752d-4183-9656-2b6d7938922d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##10. Do a final exercise of defining one/two liner of... \n",
    "1. When to use/benifits csv\n",
    "2. When to use/benifits json\n",
    "3. When to use/benifit orc\n",
    "4. When to use/benifit parquet\n",
    "5. When to use/benifit delta\n",
    "6. When to use/benifit xml\n",
    "7. When to use/benifit delta tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b981a042-913e-4e99-8917-f8e0c0a89ef1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**1. CSV – When to use / Benefits**\n",
    "\n",
    "Use CSV for **_simple data exchange_** and quick viewing because it is human-readable, lightweight, and supported everywhere, but it is not suitable for large-scale processing.\n",
    "\n",
    "**2. JSON – When to use / Benefits**\n",
    "\n",
    "Use JSON for **_semi-structured_** data and **_API integrations_** because it supports nested and flexible schemas, making it ideal for event and application data.\n",
    "\n",
    "**3. ORC – When to use / Benefit**s\n",
    "\n",
    "Use ORC for **_large data_**, read-heavy analytical workloads because it provides **_excellent compression, fast reads, and efficient predicate pushdown._**\n",
    "\n",
    "**4. Parquet – When to use / Benefi**ts\n",
    "\n",
    "Use Parquet for **_big-data analytics_** across multiple platforms because it is a **_columnar, compressed, and widely supported format_** that improves query performance.\n",
    "\n",
    "**5. Delta – When to use / Benefits**\n",
    "\n",
    "Use Delta for **_reliable data lake processing_** because it adds **_ACID transactions, schema enforcement, time travel, and scalable incremental loads_** on top of Parquet.\n",
    "\n",
    "**6. XML – When to use / Benefits**\n",
    "\n",
    "Use XML for **_legacy systems and structured data exchange_** where strict schemas and hierarchical data representation are required.\n",
    "\n",
    "**7. Delta Tables – When to use / Benefits**\n",
    "\n",
    "Use Delta tables for production-grade lakehouse architectures because they **_ensure data consistency, versioning, and optimized performance_** for large datasets.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7228358799715451,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "3.Write_usecases_workout",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
