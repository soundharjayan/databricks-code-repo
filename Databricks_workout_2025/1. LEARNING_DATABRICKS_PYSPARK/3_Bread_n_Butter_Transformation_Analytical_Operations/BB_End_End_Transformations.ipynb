{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26d7f53f-32c8-40ba-982d-04c7548932b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Very Important Spark Learning - BY LEARNING this PROGRAM - WE BECOME A DATA ENGINEER (DATA CURATION DEVELOPER & DATA ANALYST)\n",
    "Simply say- We are going to learn...\n",
    "SQL, Python Function based programming, Datawarehouse, Curation, Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37a76be2-1041-4fc6-b94b-ab46b78e169c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##**1. Data Munging** - (Cleanup) Process of transforming and mapping data from Raw form into Tidy(usable) format with the intent of making it more appropriate and valuable for a variety of downstream purposes such for further Transformation/Enrichment, Egress/Outbound, analytics, Datascience/AI application & Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9435574f-1e19-4ddd-acc4-b5f11c5fcd5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Passive Munging** - Data Discovery/Data Exploration/ EDA (Exploratory Data Analytics) (every layers ingestion/transformation/analytics/consumption) - Performing an (Data Exploration) exploratory data analysis of the raw data to identify the attributes and patterns. <br>\n",
    "\n",
    "**Active Munging** -\n",
    "1. Combining Data + Schema Evolution/Merging (Structuring) <br>\n",
    "2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy) <br>\n",
    "3. Standardization, De Duplication and Replacement & Deletion of Data to make it in a usable format (Dataengineers/consumers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc009f1a-5864-4b7b-8697-1dd72790eb62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###a. Passive Munging - \n",
    "- Data Discovery/Data Exploration/ EDA (Exploratory Data Analytics) (every layers ingestion/transformation/analytics/consumption) - Performing an (Data Exploration) exploratory data analysis of the raw data to identify the attributes and patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b76842c2-34fc-43a7-8138-e331de2af1cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Visibily/Manually opening the file we found couple of data patterns.\n",
    "-no header\n",
    "- null columns/null rows\n",
    "- extra columns/lesser number of columns\n",
    "- literal(record level duplicates)\n",
    "- column level duplicates\n",
    "- number format issue\n",
    "- single data file from single source system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88ccadc2-69ed-4744-8673-b83a29cd0eaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Programatically lets try to find couple of data patterns.\n",
    "- a. Inferschema + printSchema() is revealing the schema of the csv file, where custid is string and age is string\n",
    "- b. Structural understanding of the dataframe using printSchema, schema, dtypes, columns\n",
    "- c. describe/summary function to find the data patters\n",
    "- - howmany null values are there in every columns\n",
    "- - how the distribution/cardinality of number data happens (using percentile)\n",
    "- - other statistical info such as min, max, stddev, mean (mid value)\n",
    "- d. Duplicate records check - unique records or unique keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2665599-4ecd-4098-9a5d-cf736137a289",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766300294076}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Inferschema + printSchema() is revealing the schema of the csv file, where custid is string and age is string\n",
    "df=spark.read.csv(path=\"/Volumes/workspace/default/volumewe47_datalake/we47_source/custsmodified\",header=False,inferSchema=True).toDF(\"custid\",\"fname\",\"lname\",\"age\",\"profession\")\n",
    "display(df.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8ccd699-0d49-4203-a526-4d68fc4f6ecc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Structural understanding of the dataframe\n",
    "df.printSchema()\n",
    "print(df.schema)#When we can use this function? For defining custom schema without much efforts\n",
    "print(df.columns)#We use this to dynamically build sql queries.\n",
    "print(df.dtypes)#We can use this to identify the datatype alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87318425-70f0-403d-8b54-5d4652597e90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Statistical/summary data exploration\n",
    "#describe/summary function to find the data patters\n",
    "#howmany null values are there in every columns\n",
    "#how the distribution/cardinality of number data happens (using percentile)\n",
    "#other statistical info such as min, max, stddev, mean (mid value)\n",
    "display(df.count())\n",
    "display(df.describe())\n",
    "display(df.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd2373b0-0643-4ca9-853b-e58cffae4cf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Based on the Duplicate passive records, we can find the total number of records and the number of unique records or unique keys\n",
    "print(\"total actual records count\",df.count())\n",
    "print(\"total deduplicated records count\",df.distinct().count())\n",
    "print(\"total deduplicated id column count\",df.dropDuplicates(subset=[\"custid\"]).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ccb7e0e-aeec-4789-aecd-61187cacd34b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Active Data Munging**\n",
    "1. Combining Data + Schema Evolution/Merging/Merging (Structuring)\n",
    "2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)\n",
    "3. Standardization, De Duplication and Replacement & Deletion of Data to make it in a usable format (Dataengineers/consumers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "150ede46-5fd8-407a-930f-39392e1841ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Combining Data - If the data is present in multiple locations\n",
    "df1=spark.read.csv(\"/Volumes/workspace/default/volumewe47_datalake/we47_source/\",pathGlobFilter=\"custsmod*\",recursiveFileLookup=True,header=False,inferSchema=True)\n",
    "print(df1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c37c119-22e7-4f30-8d57-7c7a4f0908e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#What if we get customer data from multiple sources with different structure, but we need to process them together.\n",
    "#We can do Schema evolution/merging/melting on the dataframe, not only at the time of ingestion\n",
    "df1_ny=spark.read.csv(\"/Volumes/workspace/default/volumewe47_datalake/we47_source/custsmodified_NY\",header=False,inferSchema=True).toDF(\"custid\",\"fname\",\"lname\",\"age\",\"profession\")\n",
    "df2_tx=spark.read.csv(\"/Volumes/workspace/default/volumewe47_datalake/we47_source/custsmodified_TX\",header=False,inferSchema=True).toDF(\"custid\",\"fname\",\"age\",\"profession\",\"city\")\n",
    "#df1_ny.show()\n",
    "#df2_tx.show()\n",
    "#df1_df2=df1_ny.union(df2_tx)#We can't do union, because number/order/datatype of columns are different\n",
    "df1_df2=df1_ny.unionByName(df2_tx,allowMissingColumns=True)#I made the NY & TX data ready for consumption further\n",
    "df1_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6e179b5-c548-4467-a873-36413de8c5e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "struct1=StructType([StructField(\"custid\",StringType(),True),StructField(\"fname\",StringType(),True),StructField(\"lname\",StringType(),True),StructField(\"age\",StringType(),True),StructField(\"profession\",StringType(),True)])\n",
    "unclean_df=spark.read.schema(struct1).csv(path=\"/Volumes/workspace/default/volumewe47_datalake/we47_source/custsmodified\")\n",
    "#print(df.schema)\n",
    "display(df.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae22d43f-d8da-4436-9ac6-4917e7283a50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "All munging we have to do on this data next week..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62bbe034-7274-45e6-ae89-9f83243aeea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. how to create pipelines using different data processing techniques by connecting with different sources/targets\n",
    "2. how to Standardize/Modernization/Industrializing the code and how create/consume generic/reusable functions & frameworks\n",
    "3. Testing (Unit, Peer Review, SIT/Integration, Regression, User Acceptance Testing), Masking engine,\n",
    "4. Reusable transformation(munge_data, optimize_performance),\n",
    "5. Quality suite/Data Profiling/Audit engine (Reconcilation) (Audit framework), Data/process Observability\n",
    "\n",
    "6. how terminologies/architecture/submit jobs/monitor/log analysis/packaging and deployment ...\n",
    "7. performance tuning\n",
    "8. Deploying spark applications in Cloud & other Distributions like Hortonworks/Cloudera/Databricks\n",
    "9. Creating cloud pipelines using spark SQL programs & Cloud native tools\n",
    "\n",
    "What is the importance of learning this program or How this can address interview questions..?\n",
    "VERY VERY IMPORTANT PROGRAM IN TERMS OF EXPLAINING/SOLVING PROBLEMS GIVEN IN INTERVIEW ,\n",
    "WITH THIS ONE PROGRAM YOU CAN COVER ALMOST ALL DATAENGINEERING FEATURES\n",
    "Tell me about the common transformations you performed,\n",
    "tell me your daily roles in DE,\n",
    "tell me some business logics you have writtened recently\n",
    "How do you write an entire spark application,\n",
    "levels/stages of DE pipelines or\n",
    "have you created DE pipelines what are the transformations applied,\n",
    "how many you have created or are you using existing framework or you created some framework?\n",
    "\n",
    "'''\n",
    "TRANSFORMATION & ANALYTICAL TECHNIQUES\n",
    "Starting point - (Data Governance (security) - Tagging, categorization, classification, masking/filteration)\n",
    "1. Data Munging - Process of transforming and mapping data from Raw form into Tidy(usable) format with the\n",
    "intent of making it more appropriate and valuable for a variety of downstream purposes such for\n",
    "further Transformation/Enrichment, Egress/Outbound, analytics, model application & Reporting\n",
    "a. Passive - Data Discovery EDA (Exploratory Data Analytics)\n",
    "(every layers ingestion/transformation/analytics/consumption) -\n",
    "Performing an (Data Exploration) exploratory data analysis of the raw data to identify the attributes and patterns.\n",
    "b. Active - Combining Data + Schema Evolution/Merging (Structuring)\n",
    "c. Validation, Cleansing, Scrubbing - Identifying and filling gaps & Cleaning data to remove outliers and inaccuracies\n",
    "Preprocessing, Preparation\n",
    "Cleansing (removal of unwanted datasets eg. na.drop),\n",
    "Scrubbing (convert of raw to tidy na.fill or na.replace),\n",
    "d. Standardization, De Duplication and Replacement & Deletion of Data to make it in a usable format (Dataengineers/consumers)\n",
    "\n",
    "2. Data Enrichment - Makes your data rich and detailed\n",
    "a. Add, Remove, Rename, Modify/replace\n",
    "b. split, merge/Concat\n",
    "c. Type Casting, format & Schema Migration\n",
    "\n",
    "3. Data Customization & Processing - Application of Tailored Business specific Rules\n",
    "a. User Defined Functions\n",
    "b. Building of Frameworks & Reusable Functions\n",
    "\n",
    "4. Data Curation\n",
    "a. Curation/Transformation\n",
    "b. Analysis/Analytics & Summarization -> filter, transformation, Grouping, Aggregation/Summarization\n",
    "\n",
    "5. Data Wrangling - Gathering, Enriching and Transfomation of pre processed data into usable data\n",
    "a. Lookup/Reference\n",
    "b. Enrichment\n",
    "c. Joins\n",
    "d. Sorting\n",
    "e. Windowing, Statistical & Analytical processing\n",
    "f. Set Operation\n",
    "\n",
    "6. Data Publishing & Consumption - Enablement of the Cleansed, transformed and analysed data as a Data Product.\n",
    "a. Discovery,\n",
    "b. Outbound/Egress,\n",
    "c. Reports/exports\n",
    "d. Schema migration\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01d9b849-607e-4852-9687-b41332caa867",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BB_End_End_Transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
