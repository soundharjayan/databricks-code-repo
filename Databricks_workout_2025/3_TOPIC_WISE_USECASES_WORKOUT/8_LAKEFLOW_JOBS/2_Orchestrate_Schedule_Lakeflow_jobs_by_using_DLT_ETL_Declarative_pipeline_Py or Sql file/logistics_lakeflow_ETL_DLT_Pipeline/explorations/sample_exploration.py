# Databricks notebook source
# MAGIC %md
# MAGIC ### Example Exploratory Notebook
# MAGIC
# MAGIC Use this notebook to explore the data generated by the pipeline in your preferred programming language.
# MAGIC
# MAGIC **Note**: This notebook is not executed as part of the pipeline.

# COMMAND ----------

import sys

sys.path.append("/Workspace/Users/soundharjayan@gmail.com/databricks-code-repo/Databricks_workout_2025/3_TOPIC_WISE_USECASES_WORKOUT/8_LAKEFLOW_JOBS/2_Orchestrate_Schedule_Lakeflow_jobs_by_using_DLT_ETL_Declarative_pipeline_Py or Sql file/logistics_lakeflow_ETL_DLT_Pipeline")

# COMMAND ----------

# !!! Before performing any data analysis, make sure to run the pipeline to materialize the sample datasets. The tables referenced in this notebook depend on that step.

display(spark.sql("SELECT * FROM soundhar_catalog.lakeflow_job_schema.sample_aggregation_logistics_lakeflow_etl_dlt_pipeline"))
