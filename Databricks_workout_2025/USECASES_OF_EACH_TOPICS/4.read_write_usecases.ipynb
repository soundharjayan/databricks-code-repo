{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ba86a20-5a3a-4130-86f5-e312f4a7901b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Telecom Domain ReadOps Assignment\n",
    "This notebook contains assignments to practice Spark read options and Databricks volumes. <br>\n",
    "Sections: Sample data creation, Catalog & Volume creation, Copying data into Volumes, Path glob/recursive reads, toDF() column renaming variants, inferSchema/header/separator experiments, and exercises.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "841c7ed8-ef18-486a-8187-07685e499b84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](https://fplogoimages.withfloats.com/actual/68009c3a43430aff8a30419d.png)\n",
    "![](https://theciotimes.com/wp-content/uploads/2021/03/TELECOM1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4aa0a44-8cd6-41cf-921d-abb5ff67615b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##First Import all required libraries & Create spark session object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0b67823-2e4e-45e2-aa25-80550a3ac580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1. Write SQL statements to create:\n",
    "1. A catalog named telecom_catalog_assign\n",
    "2. A schema landing_zone\n",
    "3. A volume landing_vol\n",
    "4. Using dbutils.fs.mkdirs, create folders:<br>\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\n",
    "5. Explain the difference between (Just google and understand why we are going for volume concept for prod ready systems):<br>\n",
    "a. Volume vs DBFS/FileStore<br>\n",
    "b. Why production teams prefer Volumes for regulated data<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e70746c0-34ad-4ba5-b47b-9f71b24472f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "CREATE CATALOG IF NOT EXISTS telecom_catalog_assign;\n",
    "CREATE SCHEMA IF NOT EXISTS telecom_catalog_assign.landing_zone;\n",
    "CREATE VOLUME telecom_catalog_assign.landing_zone.landing_vol;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04e630c1-a58b-4cf8-8251-a1b0ea039eb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs('/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer')\n",
    "dbutils.fs.mkdirs('/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage')\n",
    "dbutils.fs.mkdirs('/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/')\n",
    "dbutils.fs.mkdirs('/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f82b5272-b994-41be-881f-a0316c276a8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We can create multiple folders in one go by using a loop, instead of writing dbutils.fs.mkdirs() one by one.\n",
    "\n",
    "folders = ['/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer','/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage','/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1 ','/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2']\n",
    "\n",
    "for folder in folders:\n",
    "  dbutils.fs.mkdirs(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa451bf9-c2e9-4863-9e88-619049f7753b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1.5 Explain the difference between (Just google and understand why we are going for volume concept for prod ready systems):**\n",
    "\n",
    "**1. Volume Vs DBFS/filestore**\n",
    "\n",
    "**DBFS/FileStore** is a legacy, workspace-level storage mainly used for temporary files, testing, or demos. It does not provide governance, fine-grained access control, or auditing, so it is **not recommended for production use**.\n",
    "\n",
    "**Databricks Volumes**, on the other hand, are a **Unity Catalog–governed storage layer** used to store files securely in cloud storage. Volumes support **fine-grained permissions, auditing, and external locations**, making them production-ready and enterprise-grade. \n",
    "\n",
    "**2. Why production teams prefer Volumes for regulated data?**\n",
    "\n",
    "Production teams prefer **Volumes** because they provide **better security, control, and tracking** for sensitive or regulated data.\n",
    "- **Only authorized people can access the data** (user/group permissions)\n",
    "- **Every access is tracked and audited**\n",
    "-** Data is stored securely in cloud storage**\n",
    "- **Access rules are centrally managed using Unity Catalog**\n",
    "\n",
    "Because of this, Volumes help organizations **follow compliance rules** (like GDPR, HIPAA, etc.) and **avoid data misuse or leaks**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26d8bd3d-b575-448b-ae22-8173d15ca671",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2.Data files to use in this usecase:\n",
    "customer_csv = '''\n",
    "101,Arun,31,Chennai,PREPAID\n",
    "102,Meera,45,Bangalore,POSTPAID\n",
    "103,Irfan,29,Hyderabad,PREPAID\n",
    "104,Raj,52,Mumbai,POSTPAID\n",
    "105,,27,Delhi,PREPAID\n",
    "106,Sneha,abc,Pune,PREPAID\n",
    "'''\n",
    "\n",
    "usage_tsv = '''customer_id\\tvoice_mins\\tdata_mb\\tsms_count\n",
    "101\\t320\\t1500\\t20\n",
    "102\\t120\\t4000\\t5\n",
    "103\\t540\\t600\\t52\n",
    "104\\t45\\t200\\t2\n",
    "105\\t0\\t0\\t0\n",
    "'''\n",
    "\n",
    "tower_logs_region1 = '''event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54\n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cf51808-8f94-4827-91a3-d728f2a00d0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_csv = '''\n",
    "101,Arun,31,Chennai,PREPAID\n",
    "102,Meera,45,Bangalore,POSTPAID\n",
    "103,Irfan,29,Hyderabad,PREPAID\n",
    "104,Raj,52,Mumbai,POSTPAID\n",
    "105,,27,Delhi,PREPAID\n",
    "106,Sneha,abc,Pune,PREPAID\n",
    "'''\n",
    "usage_tsv = '''customer_id\\tvoice_mins\\tdata_mb\\tsms_count\n",
    "101\\t320\\t1500\\t20\n",
    "102\\t120\\t4000\\t5\n",
    "103\\t540\\t600\\t52\n",
    "104\\t45\\t200\\t2\n",
    "105\\t0\\t0\\t0\n",
    "'''\n",
    "tower_logs_region1 = '''event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54\n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9540d2e2-2562-4be7-897f-0a7d57adaa72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Filesystem operations\n",
    "1. Write code to copy the above datasets into your created Volume folders:\n",
    "Customer → /Volumes/.../customer/\n",
    "Usage → /Volumes/.../usage/\n",
    "Tower (region-based) → /Volumes/.../tower/region1/ and /Volumes/.../tower/region2/\n",
    "\n",
    "2. Write a command to validate whether files were successfully copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "636f7a26-7d26-4296-b49c-a6c1110cc5cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3.1. Write code to copy the above datasets into your created Volume folders:\n",
    "'''Customer → /Volumes/.../customer/\n",
    "Usage → /Volumes/.../usage/\n",
    "Tower (region-based) → /Volumes/.../tower/region1/ and /Volumes/.../tower/region2/'''\n",
    "\n",
    "dbutils.fs.put('/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv', customer_csv, True)\n",
    "dbutils.fs.put('/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv', usage_tsv, True)\n",
    "dbutils.fs.put('/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/tower_logs_region1.csv',tower_logs_region1,True)\n",
    "dbutils.fs.put('/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2/tower_logs_region1.csv',tower_logs_region1,True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba812cd6-b104-4eb8-8b23-26d4dfb0ef19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3.2. Write a command to validate whether files were successfully copied\n",
    "\n",
    "print(dbutils.fs.ls('/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/'))\n",
    "print(dbutils.fs.ls('/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/'))\n",
    "print(dbutils.fs.ls('/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1'))\n",
    "print(dbutils.fs.ls('/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8767735b-24d3-428a-ad12-ae821903e2ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##4. Directory Read Use Cases\n",
    "1. Read all tower logs using: <br>\n",
    "Path glob filter (example: *.csv) <br>\n",
    "Multiple paths input <br>\n",
    "Recursive lookup <br>\n",
    "\n",
    "2. Demonstrate these 3 reads separately:<br>\n",
    "Using pathGlobFilter<br>\n",
    "Using list of paths in spark.read.csv([path1, path2])<br>\n",
    "Using .option(\"recursiveFileLookup\",\"true\")<br>\n",
    "\n",
    "3. Compare the outputs and understand when each should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46661e0e-272b-422c-96d3-9769cd61143f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4.1. Read all tower logs using Path glob filter (example: *.csv),Multiple paths input,Recursive lookup:\n",
    "tower_df1 = spark.read.csv([\n",
    "  \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1\",\n",
    "  \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2\"], \n",
    "  header=True, \n",
    "  inferSchema=True, \n",
    "  sep=\"|\", \n",
    "  pathGlobFilter=\"*.csv\", \n",
    "  recursiveFileLookup=True\n",
    "  )\n",
    "display(tower_df1)\n",
    "\n",
    "# 4.2. Demonstrate these 3 reads separately:\n",
    "\n",
    "# 4.2.1. Using pathGlobFilter only\n",
    "tower_df2 = spark.read.csv(\n",
    "  \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region*\",  # We can use a wild card (*) to load data from all the files or folders that match a path pattern.\n",
    "  header=True, \n",
    "  inferSchema=True, \n",
    "  sep=\"|\", \n",
    "  pathGlobFilter=\"*.csv\"  # We are using pathGlobFilter is to filter out only the csv files from the folder and also we can load multiple files at the same time.\n",
    "  )\n",
    "display(tower_df2)\n",
    "\n",
    "# 4.2.2. Using list of paths in spark.read.csv([path1, path2])\n",
    "tower_df3 = spark.read.csv([\n",
    "  \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/\",\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2/\"],\n",
    "  header=True, \n",
    "  inferSchema=True, \n",
    "  sep=\"|\", \n",
    "  pathGlobFilter=\"*.csv\"\n",
    "  )                     # We can mention the different paths in the list of csv and we can load the data from different folders\n",
    "display(tower_df3)\n",
    "\n",
    "# 4.2.3. .option(\"recursiveFileLookup\", \"true\") with multiple paths\n",
    "tower_df4 = spark.read.option(\n",
    "    \"header\",\"True\").option(\n",
    "      \"inferSchema\",\"True\").option(\n",
    "        \"sep\",\"|\").option(\n",
    "          \"pathGlobFilter\",\"*.csv\").option(\n",
    "            \"recursiveFileLookup\",\"true\").csv(\n",
    "              \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\")\n",
    "       # if we mention main folder name in the path and recursiveFileLookup=True, it will load all the data from the subfolders\n",
    "display(tower_df4)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f7147c1-5d58-47e1-84fe-7ebd26a217b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##5. Schema Inference, Header, and Separator\n",
    "1. Try the Customer, Usage files with the option and options using read.csv and format function:<br>\n",
    "header=false, inferSchema=false<br>\n",
    "or<br>\n",
    "header=true, inferSchema=true<br>\n",
    "2. Write a note on What changed when we use header or inferSchema  with true/false?<br>\n",
    "3. How schema inference handled “abc” in age?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "531521a8-2225-4c38-8cf3-91c1f2bf38c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5.1. Try the Customer, Usage files with the option and options using read.csv and format function:\n",
    "\n",
    "# 5.1.1. header=false, inferSchema=false [Using Customer file]\n",
    "\n",
    "#Reading the file using combination of option with format function\n",
    "customer_df1 = spark.read.option(\n",
    "    \"header\",\"false\").option(\n",
    "      \"inferSchema\",\"false\").format('csv').load(\n",
    "                  \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "display(customer_df1)\n",
    "\n",
    "#Reading the file using combination of options with format function\n",
    "customer_df2 = spark.read.options(\n",
    "    header=\"false\",inferSchema=\"false\").format('csv').load(\n",
    "                  \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "display(customer_df2)\n",
    "\n",
    "# Reading the file using combination of option with read.csv function\n",
    "customer_df3 = spark.read.option(\n",
    "    \"header\",\"false\").option(\n",
    "      \"inferSchema\",\"false\").csv(\n",
    "                  \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "display(customer_df2)\n",
    "\n",
    "#Reading the file using combination of options with read.csv function\n",
    "customer_df4 = spark.read.options(\n",
    "    header=\"true\",inferSchema=\"true\").csv(\n",
    "                  \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "display(customer_df4)\n",
    "\n",
    "\n",
    "# 5.1.2. header=true, inferSchema=true [Using Usage file]\n",
    "\n",
    "#Reading the file using combination of option with format function\n",
    "usage_df1 = spark.read.option(\n",
    "    \"header\",\"True\").option(\n",
    "      \"inferSchema\",\"True\").option(\n",
    "        \"sep\",\"\\t\").format('csv').load(\n",
    "                  \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\")\n",
    "display(usage_df1)\n",
    "\n",
    "#Reading the file using combination of options with format function\n",
    "usage_df2 = spark.read.options(\n",
    "    header=\"True\",inferSchema=\"True\",sep=\"\\t\").format('csv').load(\n",
    "                  \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\")\n",
    "display(usage_df2)\n",
    "\n",
    "# Reading the file using combination of option with read.csv function \n",
    "usage_df3 = spark.read.option(\n",
    "    \"header\",\"True\").option(\n",
    "      \"inferSchema\",\"True\").option(\n",
    "        \"sep\",\"\\t\").csv(\n",
    "                  \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\")\n",
    "display(usage_df3)\n",
    "\n",
    "#Reading the file using combination of options with read.csv function\n",
    "usage_df4 = spark.read.options(\n",
    "    header=\"True\",inferSchema=\"True\",sep=\"\\t\").csv(\n",
    "                  \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\")\n",
    "display(usage_df4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e120f63-3cc0-44a8-9661-00aaec9db5dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**5.2. Write a note on What changed when we use header or inferSchema  with true/false?**\n",
    "\n",
    "1.**header = True** ==> If the first row of the file contains column names, setting header = True tells Spark to treat the first row as the header and the remaining rows as data.\n",
    "\n",
    "2.**header = False** ==> If the file does not contain column names in the first row, setting header = False makes Spark automatically assign default column names such as c0, c1, c2, and so on.\n",
    "\n",
    "3.**inferSchema = True** ==> If we want Spark to automatically detect data types based on the column values, we can set inferSchema = True. Spark scans the data and assigns data types accordingly. This should be used cautiously because it requires scanning the data, which can impact performance.\n",
    "\n",
    "4.**inferSchema = False** ==> If inferSchema is set to False, Spark treats all column values as string data types by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06d8cbee-759f-4cf7-9425-96fa2e093f2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**5.3. How schema inference handled “abc” in age?** <b>\n",
    "\n",
    "The age column should always contain numeric values. If even one value like 'abc' appears in the age column, Spark will infer the entire column as StringType, because it cannot safely assign an IntegerType.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15d8dad0-bc63-47f1-9a90-72837cba6c4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##6. Column Renaming Usecases\n",
    "1. Apply column names using string using toDF function for customer data\n",
    "2. Apply column names and datatype using the schema function for usage data\n",
    "3. Apply column names and datatype using the StructType with IntegerType, StringType, TimestampType and other classes for towers data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ddfcc41-9347-4d67-b52c-da601df1b60a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6.1. Apply column names using string using toDF function for customer data\n",
    "\n",
    "customer_df1 = spark.read.options(\n",
    "    header=\"false\",\n",
    "    inferSchema=\"false\"\n",
    "    ).format('csv').load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\").toDF(\"customer_id\",\"first_name\",\"last_name\",\"city\",\"plan_type\")\n",
    "display(customer_df1)\n",
    "\n",
    "# 6.2. Apply column names and datatype using the schema function for usage data\n",
    "\n",
    "data_type= \"customer_id int,voice_mins int,data_mb int,sms_count int\"\n",
    "usage_df1 = spark.read.schema(data_type).options(\n",
    "  header=\"true\",\n",
    "  sep=\"\\t\"\n",
    "  ).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\").toDF(\"customer_id\",\"voice_mins\",\"data_mb\",\"sms_count\")\n",
    "display(usage_df1)\n",
    "\n",
    "# 6.3. Apply column names and datatype using the StructType with IntegerType, StringType, TimestampType and other classes for towers data\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType,TimestampType\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"event_id\",IntegerType(),True),\n",
    "  StructField(\"customer_id\",IntegerType(),True),\n",
    "  StructField(\"tower_id\",StringType(),True),\n",
    "  StructField(\"signal_strength\",IntegerType(),True),\n",
    "  StructField(\"timestamp\",TimestampType(),True)\n",
    "])\n",
    "tower_df1 = spark.read.schema(schema).options(\n",
    "  header=\"true\",sep=\"|\",pathGlobFilter=\"*.csv\", recursiveFileLookup=\"true\").format('csv').load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region*\")\n",
    "display(tower_df1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e1d6d88-7bcc-4548-a0d1-15d37f6fc0be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. More to come (stay motivated)...."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4647440960922810,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "4.read_write_usecases",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
